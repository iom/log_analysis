---
title: "IOM WebSites User Profiling"
subtitle: "Based on APACHE Logs"
date: "`r format(Sys.Date(),  '%d %B %Y')`"
output:
  iomdown::pptx_slides
---

## Objectives

1. Parse and preprocess logs into structured data.

2. Define user sessions based on time intervals.

3. Extract behavioral features (pages visited, time spent, device, etc.).

4. Apply clustering to identify personas.

5. Analyze navigation paths and page popularity.

6. Use insights to suggest website restructuring or mergers.

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE,
                      collapse = FALSE,
                      comment = "#>",
                      # fig.width = 5.5, fig.height = 4,
                      fig.retina = 2, 
                      fig.width = 9,
                      fig.asp = 0.618,
                      fig.align = "center",
                      dev = "ragg_png",
                      out.width = "90%")

```


```{r library, include=FALSE}
## Install IOM packages
#install.packages("pak")
#pak::pkg_install("iom/iomthemes")
#pak::pkg_install("iom/iomdown")

options(scipen = 999) # turn-off scientific notation like 1e+48
set.seed(1)
extrafont::loadfonts(quiet=TRUE)
library(officedown)
library(iomthemes)
library(ggplot2)
library(rvg)
library(tidyverse)
library(scales)
library(lubridate)
library(knitr)

# turn off the automatic use of showtext functionality, so that the dml function can works properly
showtext::showtext_auto(FALSE)
```




```{r dataloadfunction, include=FALSE,  message = FALSE, warning = FALSE}
#' Read a Compressed Apache Access Log File
#'
#' This function reads a compressed Apache access log file (typically in `.gz` format) 
#' and loads it into a data table. The logs follow the Common Log Format (CLF) or Combined Log Format 
#' but may be customized depending on server configurations.
#'
#' @param file_path A string specifying the path to the compressed log file.
#'
#' @return A data frame containing the parsed access log data with selected columns. 
#' The columns include:
#' - `ip`: Client's IP address
#' - `client_id`: Typically a hyphen (-), rarely used
#' - `user_id`: Authenticated user ID
#' - `timestamp`: Date and time of request to Analyze traffic patterns over time
#' , identify peak usage periods, and correlate with server performance or issues.
#' - `request`: HTTP request details. Determine the most requested resources, 
#'  analyze request methods (GET, POST, etc.), and identify potential
#'   misuse or attacks.
#' - `status`: HTTP response status codereturned to the client: Monitor 
#'  for errors (e.g., 404 Not Found, 500 Internal Server Error), 
#'  track successful requests, and identify potential issues with specific resources
#' - `size`: Response size in bytes: Analyze bandwidth usage, identify large
#'  file transfers, and optimize resource sizes.
#' - `referer`: Referring URL: Understand where traffic is coming from, 
#' identify popular referral sources, and detect potential referrer spam.
#' - `user_agent`: Client's browser and OS details: Information about the 
#' client's browser and operating system: Determine the most common browsers 
#' and devices used by visitors, optimize the website for different user agents,
#'  and detect automated bots.
#' - `vhost`: Virtual host handling the request: used to analyze traffic and 
#' performance for different hosts.
#' - `host`: Host header of the request: used to analyze traffic and 
#' performance for different hosts.
#' - `hosting_site`: Hosted site name
#' - `pid`: Process ID of the server handling the request: Debugging and 
#' performance analysis, identify issues with specific server processes.
#' - `request_time`: Processing time in milliseconds: Analyze server 
#' performance, identify slow requests, and optimize response times.
#' - `forwarded_for`: Original client IP (if forwarded by a proxy): Track the
#'  true origin of requests, useful for identifying clients behind proxies.
#' - `request_id`: Unique request identifier: Trace and debug specific requests,
#'  correlate logs across different systems.
#' - `location`: Redirect location header: : Track redirects, analyze the flow 
#' of traffic, and identify potential issues with redirection.
#'
#' @examples
#' 
#' # Read a compressed log file
#' log_data <- read_compressed_log("/path/to/access.log.gz")
#' head(log_data)
#'
#' @import dplyr
#' @export
read_compressed_log <- function(file_path) { 

  access_log <- read.table(gzfile(file_path, "rt"), 
                           sep = " ", 
                           fill = TRUE, 
                           skipNul = TRUE, 
                           stringsAsFactors = FALSE)
  
  col_names <- c("ip", "client_id", "user_id", "timestamp", "tocheck1", "request", 
                 "status", "size", "referer", "user_agent", "vhost", "host", 
                 "hosting_site", "pid", "request_time", "forwarded_for", 
                 "request_id", "location", "tocheck2")
  
  colnames(access_log) <- col_names
  
  access_log2 <- access_log |> 
    dplyr::select(c(ip, client_id, user_id, timestamp, request, status, size, 
                    referer, user_agent, vhost, host, hosting_site, pid, 
                    request_time, forwarded_for, request_id, location))
  
  return(access_log2)
}
```


```{r geodata, include=FALSE,  message = FALSE, warning = FALSE}
# Loading data
#install.packages("rgeolocate")
#devtools::install_github("ironholds/rgeolocate")
#library(rgeolocate)
#Use the GeoLite2 database to look up the country of an IP address:
# https://www.maxmind.com/en/accounts/1135970/geoip/downloads  
# Path to the downloaded GeoLite2 Country database
db_path <- here::here("ip","GeoLite2-Country.mmdb")
```


```{r data, include=FALSE,  message = FALSE, warning = FALSE}
logs <- c(
#  "data-raw/access.log-20250227-web-7732.gz",
#  "data-raw/access.log-20250227-web-5634.gz",
#  "data-raw/access.log-20250227-web-6641.gz",
  "data-raw/access.log-20250227-web-6642.gz"
)

# Load all the log files into data tables
log_dataall <- lapply(logs, read_compressed_log)

combined_log_data <- dplyr::bind_rows(log_dataall)

rm(log_dataall, logs)
```


```{r data2, include=FALSE,  message = FALSE, warning = FALSE}
## Now cleaning what did not correctly parsed...
combined_log_data_clean <- combined_log_data |>    
  dplyr::mutate(
    vhost = gsub("vhost=", "", vhost),
    host = gsub("host=", "", host),
    hosting_site = gsub("hosting_site=", "", hosting_site),
    pid = gsub("pid=", "", pid),
    request_time = gsub("request_time=", "", request_time),
    forwarded_for = gsub("forwarded_for=", "", forwarded_for),
    request_id = gsub("request_id=", "", request_id),
    location = gsub("location=", "", location), 
    timestamp = substring(gsub("\\[", "", timestamp), 1, 20),
    status_label = case_when(
      status == "103" ~ "103: Early Hints",
      status == "200" ~ "200: OK",
      status == "201" ~ "201: Created",
      status == "206" ~ "206: Partial Content",
      status == "301" ~ "301: Moved Perm",
      status == "302" ~ "302: Found",
      status == "303" ~ "303: See Other",
      status == "307" ~ "307: Temp Redirect",
      status == "400" ~ "400: Bad Request",
      status == "403" ~ "403: Forbidden",
      status == "404" ~ "404: Not Found",
      status == "406" ~ "406: Not Acceptable",
      status == "416" ~ "416: Range Not Satisfiable",
      status == "500" ~ "500: Server Error",
      status == "503" ~ "503: Service Unavailable",
      TRUE ~ as.character(status)  # fallback for any unexpected values
    )) |>

  dplyr::mutate(
    size = as.integer(size),
    request_time = as.integer(request_time),
    user_agent = stringr::str_to_lower(user_agent),
    # Convert timestamp to Date-Time format
    timestamp = as.POSIXct(timestamp, format="%d/%b/%Y:%H:%M:%S"),
    device_type = ifelse(str_detect(user_agent, 
                                        regex("mobile|iphone",
                                              ignore_case = TRUE)),
                             "mobile", "desktop"),
    # levels(as.factor(df$user_agent))
    # device_type = ifelse(str_detect(user_agent, 
    #                                     regex("mobile|iphone",
    #                                           ignore_case = TRUE)),
    #                          "mobile", "desktop"),
    referrer_type = case_when(
          str_detect(referer, "google|bing") ~ "search",
          str_detect(referer, "facebook|twitter") ~ "social",
          is.na(referer) ~ "direct",
          TRUE ~ "other"
        )) |>
  tidyr::separate(request, into = c("request_method",
                                    "request_resource",
                                    "request_version"), 
                  sep = " ")|>
  # levels(as.factor(df$request_version))
  ##Geolocat the ip...
  dplyr::mutate(iplocation_continent = as.character(rgeolocate::maxmind(ip,
                                                db_path)$continent_name),
                iplocation_country = as.character(rgeolocate::maxmind(ip,
                                                db_path)$country_name))

 

# Assuming your data frame is named 'df'
df <- combined_log_data_clean |>
      # dplyr::slice_head(n = 1000) 
      # remove all rows with size or request time over a certain threhsold
      dplyr::filter(request_time <= 725000000,
                size <= 13421772800) |>

      # remove all rows with size value NA
      dplyr::filter(!is.na(size))  |>

     # remove all rows with ip value = "-"
     dplyr::filter(ip != "-")

```




 
---

## Traffic patterns over time

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}

mygg <- ggplot(df, aes(x=timestamp)) +
  geom_histogram(binwidth=3600,
                 fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
                 color = "white") +
  labs(title="Traffic Patterns Over Time", x="Time", y="Number of Requests") +
  iomthemes::theme_iom(  font_size = 18,  grid = "Y")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---

## Device Type

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Status Codes
mygg <- ggplot(df |> dplyr::filter(   ! is.na(device_type)),
               aes(x=device_type)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  coord_flip()+
  labs(title="device_type", x="", y="") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---

## Referrer Type

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Status Codes
mygg <- ggplot(df |> dplyr::filter(   ! is.na(referrer_type)),
               aes(x=referrer_type)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  coord_flip()+
  labs(title="referrer_type", x="", y="") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

## Map

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# Get world map data from rnaturalearth
world <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf") |>
  # Remove Antarctica
  dplyr::filter(sovereignt != "Antarctica") |>
  # Transform the world map to Bertin 1953 projection
  # sf::st_transform(world, crs = "+proj=bertin1953")
  # Transform the world map to Robinson projection
  sf::st_transform(world, crs = "ESRI:54030")

# # Calculate centroids for each country
# world_centroids <- sf::st_centroid(world) %>%
#   # Extract coordinates from the geometry column
#   mutate(long = sf::st_coordinates(.)[,1],
#          lat = sf::st_coordinates(.)[,2])
## Better alternative
centroid <- read.csv("https://github.com/gavinr/world-countries-centroids/raw/refs/heads/master/dist/countries.csv")

## Remove centroid for sub territories
centroid.unique <- centroid |>
  dplyr::filter( ISO == AFF_ISO )|>
  dplyr::filter( COUNTRY != "Canarias" ) |>
  # Convert to an sf object
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326)  |>
  # Transform the coordinates
  sf::st_transform(crs = "ESRI:54030") |>
# Extract the transformed coordinates back into separate columns
#centroid.unique1 <- centroid.unique
  # Extract the transformed coordinates back into separate columns
  dplyr::mutate(long = sf::st_coordinates(geometry)[,1],
                lat = sf::st_coordinates(geometry)[,2])


## Cehcking we are good
#duplicated(centroid.unique$ISO)
#nrow(centroid.unique)
#length(unique(centroid.unique$ISO))


data.unique <- df |>
  dplyr::group_by(iplocation_continent, iplocation_country ) |>
  dplyr::summarise(total = dplyr::n()) |>
  dplyr::ungroup() |>
  dplyr::mutate(ISO = countrycode::countrycode(iplocation_country,
                                         origin = "country.name",
                                         destination ="iso2c" )) |>
  dplyr::arrange(desc(total))

 

## Checking
#nrow(data.unique)
#(unique(data.unique$ISO))

data.centroid <- data.unique  |>
  # Merge the sample data with the centroids data
  left_join(centroid.unique, by = c( "ISO")) |>
  dplyr::filter(! (is.na(ISO)))


# Create natural breaks for the value classes
breaks <- classInt::classIntervals(data.centroid$total, n = 5, style = "jenks")$brks
labels <- paste0("(", round(breaks[-length(breaks)], 1), " - ", round(breaks[-1], 1), "]")

label_top_10 <-  data.centroid  |>
                 dplyr::arrange(desc(total)) |> 
                dplyr::slice_head(n = 10) |>
                dplyr::mutate(
                  label = paste0(
                    ISO, ": ", 
                    scales::comma(total) # Safer alternative
                  )
                )
 
 
# Plot the world map with proportional symbols at centroids
map <- ggplot(data = world) +
  geom_sf(fill = "lightgray", alpha = 0.8, color = "white") +
  geom_point(data = data.centroid,
             aes(x = long,
                 y = lat,
                 size = total),
             color = "#0033A0", alpha = 0.5) +
  ggrepel::geom_label_repel(data = label_top_10,  
                            aes(x = long,
                                y = lat,
                                # label = scales::label_number(accuracy = 1, scale_cut = cut_short_scale())(total)) 
                                label = label)    ) +
  scale_size_continuous(range = c(1, 20),breaks = 1:5, labels = labels) + # 
  labs(
    title = "Web Traffic",
    subtitle = paste0("Number of hits: ",
                      scales::label_number(accuracy = 1,   scale_cut = cut_short_scale()) (sum(data.centroid$total)),
                      ""),
    x="",
    y = "",
    caption = "Source: IOM Access Logs",
    size = "Value") +
  coord_sf() +
  iomthemes::theme_iom(void = TRUE)


dml(ggobj = map)

```



---

## Country for IP

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Status Codes
mygg <- df    |>
  dplyr::mutate(iplocation_country2 = fct_lump_prop(iplocation_country , prop = 0.01 , other_level = "Other lumped (<10% total)" ))   |>
  dplyr::group_by(iplocation_country2 ) |>
  dplyr::summarise(total = dplyr::n()) |>
  dplyr::filter( ! ( is.na(iplocation_country2))) |>
  ggplot() +
  aes(x = reorder(iplocation_country2,total),
      y = total ) +
  geom_col( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  coord_flip()+
  labs(title="Country for IP", x="", y="") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---

## Region for IP

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Status Codes
mygg <- ggplot(df |> dplyr::filter(   ! is.na(iplocation_continent)),
               aes(x=iplocation_continent)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  coord_flip()+
  labs(title="Region for IP", x="", y="") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---

## HTTP Status Codes

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Status Codes
mygg <- ggplot(df |> dplyr::filter( status != "200"),
               aes(x=status_label)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  coord_flip()+
  labs(title="HTTP Status Codes", x="Status Code", y="Count") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---

## HTTP Methods Distribution

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Methods Distribution
mygg <- ggplot(df, aes(x=request_method)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  labs(title="HTTP Methods Distribution", x="HTTP Method", y="Count") +
  iomthemes::theme_iom(  font_size = 18,  grid = "Y")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```


---

## HTTP Status Codes

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# HTTP Status Codes
mygg <- ggplot(df |> dplyr::filter( status != "200"),
               aes(x=status)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
            color = "white") +
  coord_flip()+
  labs(title="HTTP Status Codes", x="Status Code", y="Count") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```


---

## Bandwidth Usage by Request Size

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# Bandwidth Usage by Request Size
mygg <- ggplot(df |> dplyr::filter(size <10000), 
               aes(x=size)) +
      # binwidth estimate based on Sturges' Rule
      geom_histogram(#binwidth=20000000, 
                 fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
                 color = "white") +
  labs(title="Bandwidth Usage by Request Size",
       x="Size (bytes)",
       y="Count") +
  iomthemes::theme_iom(  font_size = 18,  grid = "Y")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```


---

## Request Time Analysis

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# Request Time Analysis
mygg <- ggplot(df|> dplyr::filter(request_time <100000), 
                aes(x=request_time)) +
  # # binwidth estimate based on Sturges' Rule
  geom_histogram(#binwidth=25000000, 
                 fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
                 color = "white") +
  labs(title="Request Time Analysis", 
       x="Request Time (ms)",
       y="Count") +
  iomthemes::theme_iom(  font_size = 18,  grid = "Y")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---

## Traffic by User Agent

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# Traffic by User Agent
mygg <- ggplot(df, 
               aes(x=user_agent)) +
  geom_bar( fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
                 color = "white") +
#  coord_flip()+
  labs(title="Traffic by User Agent", x="User Agent", y="Count") +
  iomthemes::theme_iom(  font_size = 18,  grid = "Y") +
  theme(axis.text.x = element_text(angle = 0, hjust = 15)) 


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

 

---

## Traffic by  Host

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}
# Traffic by Virtual Host
mygg <-  df    |>
  dplyr::mutate(host2 = fct_lump_prop(host , prop = 0.005 , other_level = "Other lumped (<5% total)" ))   |>
  dplyr::group_by(host2 ) |>
  dplyr::summarise(total = dplyr::n()) |>
  dplyr::filter( ! ( is.na(host2))) |>
  ggplot() +
  aes(x = reorder(host2,total),
      y = total ) + 
  geom_col(
                 fill= iomthemes::iom_pal(5, "pal_blue")[1], ## IOM blue
                 color = "white") +
  coord_flip()+
  labs(title="Traffic by  Host", x=" ", y=" ") +
  iomthemes::theme_iom(  font_size = 18,  grid = "X")  


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```


##  Detecting Security Threats

Monitor the requests that indicate hack attempts:
 * "GET /?q=node/add HTTP/1.1",
 * "GET /?q=user/register HTTP/1.1",
 * "GET /?q=node/add HTTP/1.0", and
 * "GET /?q=user/register HTTP/1.0" .

if there are many hacking attempts, We can identify the IP that had the highest number of hacking attempts in order to add it to the firewall's deny list.
 
---

##  Define User Sessions

Group requests into sessions to analyze user journeys. A session can be defined as a sequence of requests from the same IP with no more than 30 minutes of inactivity.

```{r session, include=FALSE,  message = FALSE, warning = FALSE}
log_df <- df |> 
  # Filter successful requests
  dplyr::filter(status == "200") |> 
  dplyr::arrange(ip, timestamp) |>
  dplyr::group_by(ip) |>
  dplyr::summarise( time_diff = difftime(timestamp, lag(timestamp)) ) #, 
          # 30 minutes = 1800 seconds
  # dplyr::mutate( new_session = coalesce(time_diff > 1800, TRUE)    ) |>
  # dplyr::mutate(session_id = cumsum(new_session)) |>
  # dplyr::ungroup()
```

---

## Feature Engineering 
    
Extract behavioral features to characterize users:
      
- Frequency: Visits per user.
- Recency: Time since last visit.
- Content Preferences: Most-visited pages.
- Device Type: Mobile vs. desktop.
- Referral Source: Direct, search, or social.

```{r usersession, include=FALSE,  message = FALSE, warning = FALSE}
user_features <- log_sessions |>
      group_by(ip, session_id) |>
      summarise(
        session_start = min(timestamp),
        session_end = max(timestamp),
        session_duration = as.numeric(difftime(session_end,
                                               session_start,
                                               units = "secs")),
        pages_visited = n(),
        unique_pages = n_distinct(request_resource),
        top_page = names(which.max(table(request_resource)))
      )
```

---

## User Persona Inference

```{r userpersonna, include=FALSE,  message = FALSE, warning = FALSE}
#Use clustering (e.g., k-means or hierarchical clustering) to group users into personas based on their behavior.

# Select features for clustering
clustering_data <- user_features |>
      select(session_duration, pages_visited, unique_pages, device_type, referrer_type) |>
      mutate(device_type = as.factor(device_type), referrer_type = as.factor(referrer_type))
    
# Convert factors to numeric (simplified example)
clustering_data <- model.matrix(~ . -1, data = clustering_data)
#library(Matrix)
#clustering_data <- sparse.model.matrix(~ . -1, data = clustering_data) #use sparse matrix to store only no zero value
    
# Scale data
clustering_data_scaled <- scale(clustering_data)
# clustering_data_scaled <- clustering_data
# for (col in names(clustering_data)) {
#   clustering_data_scaled[[col]] <- scale(clustering_data[[col]])
# }
    
# Perform k-means (e.g., 4 personas)
set.seed(123)
kmeans_result <- kmeans(clustering_data_scaled, centers = 4)
# library(skmeans)
# kmeans_result <- skmeans(clustering_data_scaled, k = 4)

# Assign clusters to users
user_features$persona <- kmeans_result$cluster
    
# Persona Profiles
persona_profiles <- user_features |>
      group_by(persona) |>
      summarise(
        avg_duration = mean(session_duration),
        avg_pages = mean(pages_visited),
        common_device = names(which.max(table(device_type))),
        common_referrer = names(which.max(table(referrer_type))),
        sample_size = n()
      )
    
# Output
persona_profiles
```

 

---

## Profiles

```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}

mygg <- ggplot(persona_profiles, aes(x = persona, y = sample_size, fill = common_device)) +
  geom_col() +
  labs(title = "User Persona Distribution", x = "Persona", y = "Users")


dml(ggobj = mygg, fonts = list(serif = 'Lato'))
```

---
##  Website Restructuring/Merging Insights  


```{r layout='Title and Content', ph=officer::ph_location_type(type="body")}

###  Content Popularity Analysis  ######
page_analysis <- log_df |>
  count(request_resource) |>
  mutate(pct = n / sum(n)) |>
  arrange(desc(n))

# Top 10 pages
page_analysis |> slice_head(n = 10)
# Heatmaps: heatmaply to show page popularity.

```

---

##  Navigation Path Analysis 

Use the personas and navigation patterns to guide restructuring:

```{r userpersonna, include=FALSE,  message = FALSE, warning = FALSE}
### B. Navigation Path Analysis ######

library(clickstream)

# Convert sessions to clickstreams
sessions <- log_sessions |>
  group_by(session_id) |>
  summarise(path = paste(request_resource, collapse = " > "))

library(clickstream)

# Convert sessions to clickstreams
sessions <- log_sessions %>%
  group_by(session_id) %>%
  summarise(path = paste(requested_page, collapse = " > "))

# Build Markov chain model
clickstreams <- as.clickstreams(sessions$path)
mc <- fitMarkovChain(clickstreams)

# Plot common paths
plot(mc)
```

---

# Potential Actionable Recommendations 

- Merge Low-Traffic Pages: Combine pages with <1% traffic into broader categories.

- Merge Blog and Articles Sections:  If /blog and /articles have overlapping content, combine them into /resources.

- Improve Mobile Navigation: Shorten paths for mobile users (e.g., fewer clicks to checkout).

- Personalize Homepage: Redirect search users (Persona 2/4) to in-depth guides &  social users (Persona 1) to visual content (videos/infographics).

- Optimize Navigation for Personas:  Simplify paths for mobile users (Persona 1/3) & Add cross-links between related content for engaged users (Persona 4).

- SEO/Social Alignment: - Tailor content for search-driven personas (Persona 2/4) &  Add social sharing buttons for Persona 1.




